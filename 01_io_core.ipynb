{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f829a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp io.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\"\"\"This module provides a set of classes which underpin the data loading and\n",
    "saving functionality provided by ``kedro.io``.\n",
    "\"\"\"\n",
    "\n",
    "import abc\n",
    "import copy\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from datetime import datetime, timezone\n",
    "from functools import partial\n",
    "from glob import iglob\n",
    "from operator import attrgetter\n",
    "from pathlib import Path, PurePath, PurePosixPath\n",
    "from typing import Any, Callable, Dict, Generic, List, Optional, Tuple, Type, TypeVar\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "from cachetools import Cache, cachedmethod\n",
    "from cachetools.keys import hashkey\n",
    "\n",
    "from kedro.utils import load_obj\n",
    "\n",
    "warnings.simplefilter(\"default\", DeprecationWarning)\n",
    "\n",
    "VERSION_FORMAT = \"%Y-%m-%dT%H.%M.%S.%fZ\"\n",
    "VERSIONED_FLAG_KEY = \"versioned\"\n",
    "VERSION_KEY = \"version\"\n",
    "HTTP_PROTOCOLS = (\"http\", \"https\")\n",
    "PROTOCOL_DELIMITER = \"://\"\n",
    "CLOUD_PROTOCOLS = (\"s3\", \"gcs\", \"gs\", \"adl\", \"abfs\", \"abfss\")\n",
    "\n",
    "\n",
    "class DataSetError(Exception):\n",
    "    \"\"\"``DataSetError`` raised by ``AbstractDataSet`` implementations\n",
    "    in case of failure of input/output methods.\n",
    "\n",
    "    ``AbstractDataSet`` implementations should provide instructive\n",
    "    information in case of failure.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class DataSetNotFoundError(DataSetError):\n",
    "    \"\"\"``DataSetNotFoundError`` raised by ``DataCatalog`` class in case of\n",
    "    trying to use a non-existing data set.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class DataSetAlreadyExistsError(DataSetError):\n",
    "    \"\"\"``DataSetAlreadyExistsError`` raised by ``DataCatalog`` class in case\n",
    "    of trying to add a data set which already exists in the ``DataCatalog``.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class VersionNotFoundError(DataSetError):\n",
    "    \"\"\"``VersionNotFoundError`` raised by ``AbstractVersionedDataSet`` implementations\n",
    "    in case of no load versions available for the data set.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "_DI = TypeVar(\"_DI\")\n",
    "_DO = TypeVar(\"_DO\")\n",
    "\n",
    "\n",
    "class AbstractDataSet(abc.ABC, Generic[_DI, _DO]):\n",
    "    \"\"\"``AbstractDataSet`` is the base class for all data set implementations.\n",
    "    All data set implementations should extend this abstract class\n",
    "    and implement the methods marked as abstract.\n",
    "    If a specific dataset implementation cannot be used in conjunction with\n",
    "    the ``ParallelRunner``, such user-defined dataset should have the\n",
    "    attribute `_SINGLE_PROCESS = True`.\n",
    "    Example:\n",
    "    ::\n",
    "\n",
    "        >>> from pathlib import Path, PurePosixPath\n",
    "        >>> import pandas as pd\n",
    "        >>> from kedro.io import AbstractDataSet\n",
    "        >>>\n",
    "        >>>\n",
    "        >>> class MyOwnDataSet(AbstractDataSet[pd.DataFrame, pd.DataFrame]):\n",
    "        >>>     def __init__(self, filepath, param1, param2=True):\n",
    "        >>>         self._filepath = PurePosixPath(filepath)\n",
    "        >>>         self._param1 = param1\n",
    "        >>>         self._param2 = param2\n",
    "        >>>\n",
    "        >>>     def _load(self) -> pd.DataFrame:\n",
    "        >>>         return pd.read_csv(self._filepath)\n",
    "        >>>\n",
    "        >>>     def _save(self, df: pd.DataFrame) -> None:\n",
    "        >>>         df.to_csv(str(self._filepath))\n",
    "        >>>\n",
    "        >>>     def _exists(self) -> bool:\n",
    "        >>>         return Path(self._filepath.as_posix()).exists()\n",
    "        >>>\n",
    "        >>>     def _describe(self):\n",
    "        >>>         return dict(param1=self._param1, param2=self._param2)\n",
    "\n",
    "    Example catalog.yml specification:\n",
    "    ::\n",
    "\n",
    "        my_dataset:\n",
    "            type: <path-to-my-own-dataset>.MyOwnDataSet\n",
    "            filepath: data/01_raw/my_data.csv\n",
    "            param1: <param1-value> # param1 is a required argument\n",
    "            # param2 will be True by default\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(\n",
    "        cls: Type,\n",
    "        name: str,\n",
    "        config: Dict[str, Any],\n",
    "        load_version: str = None,\n",
    "        save_version: str = None,\n",
    "    ) -> \"AbstractDataSet\":\n",
    "        \"\"\"Create a data set instance using the configuration provided.\n",
    "\n",
    "        Args:\n",
    "            name: Data set name.\n",
    "            config: Data set config dictionary.\n",
    "            load_version: Version string to be used for ``load`` operation if\n",
    "                the data set is versioned. Has no effect on the data set\n",
    "                if versioning was not enabled.\n",
    "            save_version: Version string to be used for ``save`` operation if\n",
    "                the data set is versioned. Has no effect on the data set\n",
    "                if versioning was not enabled.\n",
    "\n",
    "        Returns:\n",
    "            An instance of an ``AbstractDataSet`` subclass.\n",
    "\n",
    "        Raises:\n",
    "            DataSetError: When the function fails to create the data set\n",
    "                from its config.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            class_obj, config = parse_dataset_definition(\n",
    "                config, load_version, save_version\n",
    "            )\n",
    "        except Exception as exc:\n",
    "            raise DataSetError(\n",
    "                f\"An exception occurred when parsing config \"\n",
    "                f\"for DataSet '{name}':\\n{str(exc)}\"\n",
    "            ) from exc\n",
    "\n",
    "        try:\n",
    "            data_set = class_obj(**config)  # type: ignore\n",
    "        except TypeError as err:\n",
    "            raise DataSetError(\n",
    "                f\"\\n{err}.\\nDataSet '{name}' must only contain arguments valid for the \"\n",
    "                f\"constructor of '{class_obj.__module__}.{class_obj.__qualname__}'.\"\n",
    "            ) from err\n",
    "        except Exception as err:\n",
    "            raise DataSetError(\n",
    "                f\"\\n{err}.\\nFailed to instantiate DataSet '{name}' \"\n",
    "                f\"of type '{class_obj.__module__}.{class_obj.__qualname__}'.\"\n",
    "            ) from err\n",
    "        return data_set\n",
    "\n",
    "    @property\n",
    "    def _logger(self) -> logging.Logger:\n",
    "        return logging.getLogger(__name__)\n",
    "\n",
    "    def load(self) -> _DO:\n",
    "        \"\"\"Loads data by delegation to the provided load method.\n",
    "\n",
    "        Returns:\n",
    "            Data returned by the provided load method.\n",
    "\n",
    "        Raises:\n",
    "            DataSetError: When underlying load method raises error.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self._logger.debug(\"Loading %s\", str(self))\n",
    "\n",
    "        try:\n",
    "            return self._load()\n",
    "        except DataSetError:\n",
    "            raise\n",
    "        except Exception as exc:\n",
    "            # This exception handling is by design as the composed data sets\n",
    "            # can throw any type of exception.\n",
    "            message = (\n",
    "                f\"Failed while loading data from data set {str(self)}.\\n{str(exc)}\"\n",
    "            )\n",
    "            raise DataSetError(message) from exc\n",
    "\n",
    "    def save(self, data: _DI) -> None:\n",
    "        \"\"\"Saves data by delegation to the provided save method.\n",
    "\n",
    "        Args:\n",
    "            data: the value to be saved by provided save method.\n",
    "\n",
    "        Raises:\n",
    "            DataSetError: when underlying save method raises error.\n",
    "            FileNotFoundError: when save method got file instead of dir, on Windows.\n",
    "            NotADirectoryError: when save method got file instead of dir, on Unix.\n",
    "        \"\"\"\n",
    "\n",
    "        if data is None:\n",
    "            raise DataSetError(\"Saving 'None' to a 'DataSet' is not allowed\")\n",
    "\n",
    "        try:\n",
    "            self._logger.debug(\"Saving %s\", str(self))\n",
    "            self._save(data)\n",
    "        except DataSetError:\n",
    "            raise\n",
    "        except (FileNotFoundError, NotADirectoryError):\n",
    "            raise\n",
    "        except Exception as exc:\n",
    "            message = f\"Failed while saving data to data set {str(self)}.\\n{str(exc)}\"\n",
    "            raise DataSetError(message) from exc\n",
    "\n",
    "    def __str__(self):\n",
    "        def _to_str(obj, is_root=False):\n",
    "            \"\"\"Returns a string representation where\n",
    "            1. The root level (i.e. the DataSet.__init__ arguments) are\n",
    "            formatted like DataSet(key=value).\n",
    "            2. Dictionaries have the keys alphabetically sorted recursively.\n",
    "            3. None values are not shown.\n",
    "            \"\"\"\n",
    "\n",
    "            fmt = \"{}={}\" if is_root else \"'{}': {}\"  # 1\n",
    "\n",
    "            if isinstance(obj, dict):\n",
    "                sorted_dict = sorted(obj.items(), key=lambda pair: str(pair[0]))  # 2\n",
    "\n",
    "                text = \", \".join(\n",
    "                    fmt.format(key, _to_str(value))  # 2\n",
    "                    for key, value in sorted_dict\n",
    "                    if value is not None  # 3\n",
    "                )\n",
    "\n",
    "                return text if is_root else \"{\" + text + \"}\"  # 1\n",
    "\n",
    "            # not a dictionary\n",
    "            return str(obj)\n",
    "\n",
    "        return f\"{type(self).__name__}({_to_str(self._describe(), True)})\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _load(self) -> _DO:\n",
    "        raise NotImplementedError(\n",
    "            f\"'{self.__class__.__name__}' is a subclass of AbstractDataSet and \"\n",
    "            f\"it must implement the '_load' method\"\n",
    "        )\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _save(self, data: _DI) -> None:\n",
    "        raise NotImplementedError(\n",
    "            f\"'{self.__class__.__name__}' is a subclass of AbstractDataSet and \"\n",
    "            f\"it must implement the '_save' method\"\n",
    "        )\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _describe(self) -> Dict[str, Any]:\n",
    "        raise NotImplementedError(\n",
    "            f\"'{self.__class__.__name__}' is a subclass of AbstractDataSet and \"\n",
    "            f\"it must implement the '_describe' method\"\n",
    "        )\n",
    "\n",
    "    def exists(self) -> bool:\n",
    "        \"\"\"Checks whether a data set's output already exists by calling\n",
    "        the provided _exists() method.\n",
    "\n",
    "        Returns:\n",
    "            Flag indicating whether the output already exists.\n",
    "\n",
    "        Raises:\n",
    "            DataSetError: when underlying exists method raises error.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._logger.debug(\"Checking whether target of %s exists\", str(self))\n",
    "            return self._exists()\n",
    "        except Exception as exc:\n",
    "            message = (\n",
    "                f\"Failed during exists check for data set {str(self)}.\\n{str(exc)}\"\n",
    "            )\n",
    "            raise DataSetError(message) from exc\n",
    "\n",
    "    def _exists(self) -> bool:\n",
    "        self._logger.warning(\n",
    "            \"'exists()' not implemented for '%s'. Assuming output does not exist.\",\n",
    "            self.__class__.__name__,\n",
    "        )\n",
    "        return False\n",
    "\n",
    "    def release(self) -> None:\n",
    "        \"\"\"Release any cached data.\n",
    "\n",
    "        Raises:\n",
    "            DataSetError: when underlying release method raises error.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._logger.debug(\"Releasing %s\", str(self))\n",
    "            self._release()\n",
    "        except Exception as exc:\n",
    "            message = f\"Failed during release for data set {str(self)}.\\n{str(exc)}\"\n",
    "            raise DataSetError(message) from exc\n",
    "\n",
    "    def _release(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def _copy(self, **overwrite_params) -> \"AbstractDataSet\":\n",
    "        dataset_copy = copy.deepcopy(self)\n",
    "        for name, value in overwrite_params.items():\n",
    "            setattr(dataset_copy, name, value)\n",
    "        return dataset_copy\n",
    "\n",
    "\n",
    "def generate_timestamp() -> str:\n",
    "    \"\"\"Generate the timestamp to be used by versioning.\n",
    "\n",
    "    Returns:\n",
    "        String representation of the current timestamp.\n",
    "\n",
    "    \"\"\"\n",
    "    current_ts = datetime.now(tz=timezone.utc).strftime(VERSION_FORMAT)\n",
    "    return current_ts[:-4] + current_ts[-1:]  # Don't keep microseconds\n",
    "\n",
    "\n",
    "class Version(namedtuple(\"Version\", [\"load\", \"save\"])):\n",
    "    \"\"\"This namedtuple is used to provide load and save versions for versioned\n",
    "    data sets. If ``Version.load`` is None, then the latest available version\n",
    "    is loaded. If ``Version.save`` is None, then save version is formatted as\n",
    "    YYYY-MM-DDThh.mm.ss.sssZ of the current timestamp.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = ()\n",
    "\n",
    "\n",
    "_CONSISTENCY_WARNING = (\n",
    "    \"Save version '{}' did not match load version '{}' for {}. This is strongly \"\n",
    "    \"discouraged due to inconsistencies it may cause between 'save' and \"\n",
    "    \"'load' operations. Please refrain from setting exact load version for \"\n",
    "    \"intermediate data sets where possible to avoid this warning.\"\n",
    ")\n",
    "\n",
    "_DEFAULT_PACKAGES = [\"kedro.io.\", \"kedro.extras.datasets.\", \"\"]\n",
    "\n",
    "\n",
    "def parse_dataset_definition(\n",
    "    config: Dict[str, Any], load_version: str = None, save_version: str = None\n",
    ") -> Tuple[Type[AbstractDataSet], Dict[str, Any]]:\n",
    "    \"\"\"Parse and instantiate a dataset class using the configuration provided.\n",
    "\n",
    "    Args:\n",
    "        config: Data set config dictionary. It *must* contain the `type` key\n",
    "            with fully qualified class name.\n",
    "        load_version: Version string to be used for ``load`` operation if\n",
    "                the data set is versioned. Has no effect on the data set\n",
    "                if versioning was not enabled.\n",
    "        save_version: Version string to be used for ``save`` operation if\n",
    "            the data set is versioned. Has no effect on the data set\n",
    "            if versioning was not enabled.\n",
    "\n",
    "    Raises:\n",
    "        DataSetError: If the function fails to parse the configuration provided.\n",
    "\n",
    "    Returns:\n",
    "        2-tuple: (Dataset class object, configuration dictionary)\n",
    "    \"\"\"\n",
    "    save_version = save_version or generate_timestamp()\n",
    "    config = copy.deepcopy(config)\n",
    "\n",
    "    if \"type\" not in config:\n",
    "        raise DataSetError(\"'type' is missing from DataSet catalog configuration\")\n",
    "\n",
    "    class_obj = config.pop(\"type\")\n",
    "    if isinstance(class_obj, str):\n",
    "        if len(class_obj.strip(\".\")) != len(class_obj):\n",
    "            raise DataSetError(\n",
    "                \"'type' class path does not support relative \"\n",
    "                \"paths or paths ending with a dot.\"\n",
    "            )\n",
    "        class_paths = (prefix + class_obj for prefix in _DEFAULT_PACKAGES)\n",
    "\n",
    "        trials = (_load_obj(class_path) for class_path in class_paths)\n",
    "        try:\n",
    "            class_obj = next(obj for obj in trials if obj is not None)\n",
    "        except StopIteration as exc:\n",
    "            raise DataSetError(\n",
    "                f\"Class '{class_obj}' not found or one of its dependencies \"\n",
    "                f\"has not been installed.\"\n",
    "            ) from exc\n",
    "\n",
    "    if not issubclass(class_obj, AbstractDataSet):\n",
    "        raise DataSetError(\n",
    "            f\"DataSet type '{class_obj.__module__}.{class_obj.__qualname__}' \"\n",
    "            f\"is invalid: all data set types must extend 'AbstractDataSet'.\"\n",
    "        )\n",
    "\n",
    "    if VERSION_KEY in config:\n",
    "        # remove \"version\" key so that it's not passed\n",
    "        # to the \"unversioned\" data set constructor\n",
    "        message = (\n",
    "            \"'%s' attribute removed from data set configuration since it is a \"\n",
    "            \"reserved word and cannot be directly specified\"\n",
    "        )\n",
    "        logging.getLogger(__name__).warning(message, VERSION_KEY)\n",
    "        del config[VERSION_KEY]\n",
    "\n",
    "    # dataset is either versioned explicitly by the user or versioned is set to true by default\n",
    "    # on the dataset\n",
    "    if config.pop(VERSIONED_FLAG_KEY, False) or getattr(\n",
    "        class_obj, VERSIONED_FLAG_KEY, False\n",
    "    ):\n",
    "        config[VERSION_KEY] = Version(load_version, save_version)\n",
    "\n",
    "    return class_obj, config\n",
    "\n",
    "\n",
    "def _load_obj(class_path: str) -> Optional[object]:\n",
    "    mod_path, _, class_name = class_path.rpartition(\".\")\n",
    "    try:\n",
    "        available_classes = load_obj(f\"{mod_path}.__all__\")\n",
    "    # ModuleNotFoundError: When `load_obj` can't find `mod_path` (e.g `kedro.io.pandas`)\n",
    "    #                      this is because we try a combination of all prefixes.\n",
    "    # AttributeError: When `load_obj` manages to load `mod_path` but it doesn't have an\n",
    "    #                 `__all__` attribute -- either because it's a custom or a kedro.io dataset\n",
    "    except (ModuleNotFoundError, AttributeError, ValueError):\n",
    "        available_classes = None\n",
    "\n",
    "    try:\n",
    "        class_obj = load_obj(class_path)\n",
    "    except (ModuleNotFoundError, ValueError):\n",
    "        return None\n",
    "    except AttributeError as exc:\n",
    "        if available_classes and class_name in available_classes:\n",
    "            raise DataSetError(\n",
    "                f\"{exc} Please see the documentation on how to \"\n",
    "                f\"install relevant dependencies for {class_path}:\\n\"\n",
    "                f\"https://kedro.readthedocs.io/en/stable/\"\n",
    "                f\"kedro_project_setup/dependencies.html\"\n",
    "            ) from exc\n",
    "        return None\n",
    "\n",
    "    return class_obj\n",
    "\n",
    "\n",
    "def _local_exists(filepath: str) -> bool:  # SKIP_IF_NO_SPARK\n",
    "    filepath = Path(filepath)\n",
    "    return filepath.exists() or any(par.is_file() for par in filepath.parents)\n",
    "\n",
    "\n",
    "class AbstractVersionedDataSet(AbstractDataSet[_DI, _DO], abc.ABC):\n",
    "    \"\"\"\n",
    "    ``AbstractVersionedDataSet`` is the base class for all versioned data set\n",
    "    implementations. All data sets that implement versioning should extend this\n",
    "    abstract class and implement the methods marked as abstract.\n",
    "\n",
    "    Example:\n",
    "    ::\n",
    "\n",
    "        >>> from pathlib import Path, PurePosixPath\n",
    "        >>> import pandas as pd\n",
    "        >>> from kedro.io import AbstractVersionedDataSet\n",
    "        >>>\n",
    "        >>>\n",
    "        >>> class MyOwnDataSet(AbstractVersionedDataSet):\n",
    "        >>>     def __init__(self, filepath, version, param1, param2=True):\n",
    "        >>>         super().__init__(PurePosixPath(filepath), version)\n",
    "        >>>         self._param1 = param1\n",
    "        >>>         self._param2 = param2\n",
    "        >>>\n",
    "        >>>     def _load(self) -> pd.DataFrame:\n",
    "        >>>         load_path = self._get_load_path()\n",
    "        >>>         return pd.read_csv(load_path)\n",
    "        >>>\n",
    "        >>>     def _save(self, df: pd.DataFrame) -> None:\n",
    "        >>>         save_path = self._get_save_path()\n",
    "        >>>         df.to_csv(str(save_path))\n",
    "        >>>\n",
    "        >>>     def _exists(self) -> bool:\n",
    "        >>>         path = self._get_load_path()\n",
    "        >>>         return Path(path.as_posix()).exists()\n",
    "        >>>\n",
    "        >>>     def _describe(self):\n",
    "        >>>         return dict(version=self._version, param1=self._param1, param2=self._param2)\n",
    "\n",
    "    Example catalog.yml specification:\n",
    "    ::\n",
    "\n",
    "        my_dataset:\n",
    "            type: <path-to-my-own-dataset>.MyOwnDataSet\n",
    "            filepath: data/01_raw/my_data.csv\n",
    "            versioned: true\n",
    "            param1: <param1-value> # param1 is a required argument\n",
    "            # param2 will be True by default\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filepath: PurePosixPath,\n",
    "        version: Optional[Version],\n",
    "        exists_function: Callable[[str], bool] = None,\n",
    "        glob_function: Callable[[str], List[str]] = None,\n",
    "    ):\n",
    "        \"\"\"Creates a new instance of ``AbstractVersionedDataSet``.\n",
    "\n",
    "        Args:\n",
    "            filepath: Filepath in POSIX format to a file.\n",
    "            version: If specified, should be an instance of\n",
    "                ``kedro.io.core.Version``. If its ``load`` attribute is\n",
    "                None, the latest version will be loaded. If its ``save``\n",
    "                attribute is None, save version will be autogenerated.\n",
    "            exists_function: Function that is used for determining whether\n",
    "                a path exists in a filesystem.\n",
    "            glob_function: Function that is used for finding all paths\n",
    "                in a filesystem, which match a given pattern.\n",
    "        \"\"\"\n",
    "        self._filepath = filepath\n",
    "        self._version = version\n",
    "        self._exists_function = exists_function or _local_exists\n",
    "        self._glob_function = glob_function or iglob\n",
    "        # 1 entry for load version, 1 for save version\n",
    "        self._version_cache = Cache(maxsize=2)  # type: Cache\n",
    "\n",
    "    # 'key' is set to prevent cache key overlapping for load and save:\n",
    "    # https://cachetools.readthedocs.io/en/stable/#cachetools.cachedmethod\n",
    "    @cachedmethod(cache=attrgetter(\"_version_cache\"), key=partial(hashkey, \"load\"))\n",
    "    def _fetch_latest_load_version(self) -> str:\n",
    "        # When load version is unpinned, fetch the most recent existing\n",
    "        # version from the given path.\n",
    "        pattern = str(self._get_versioned_path(\"*\"))\n",
    "        version_paths = sorted(self._glob_function(pattern), reverse=True)\n",
    "        most_recent = next(\n",
    "            (path for path in version_paths if self._exists_function(path)), None\n",
    "        )\n",
    "\n",
    "        if not most_recent:\n",
    "            raise VersionNotFoundError(f\"Did not find any versions for {self}\")\n",
    "\n",
    "        return PurePath(most_recent).parent.name\n",
    "\n",
    "    # 'key' is set to prevent cache key overlapping for load and save:\n",
    "    # https://cachetools.readthedocs.io/en/stable/#cachetools.cachedmethod\n",
    "    @cachedmethod(cache=attrgetter(\"_version_cache\"), key=partial(hashkey, \"save\"))\n",
    "    def _fetch_latest_save_version(self) -> str:  # pylint: disable=no-self-use\n",
    "        \"\"\"Generate and cache the current save version\"\"\"\n",
    "        return generate_timestamp()\n",
    "\n",
    "    def resolve_load_version(self) -> Optional[str]:\n",
    "        \"\"\"Compute the version the dataset should be loaded with.\"\"\"\n",
    "        if not self._version:\n",
    "            return None\n",
    "        if self._version.load:\n",
    "            return self._version.load\n",
    "        return self._fetch_latest_load_version()\n",
    "\n",
    "    def _get_load_path(self) -> PurePosixPath:\n",
    "        if not self._version:\n",
    "            # When versioning is disabled, load from original filepath\n",
    "            return self._filepath\n",
    "\n",
    "        load_version = self.resolve_load_version()\n",
    "        return self._get_versioned_path(load_version)  # type: ignore\n",
    "\n",
    "    def resolve_save_version(self) -> Optional[str]:\n",
    "        \"\"\"Compute the version the dataset should be saved with.\"\"\"\n",
    "        if not self._version:\n",
    "            return None\n",
    "        if self._version.save:\n",
    "            return self._version.save\n",
    "        return self._fetch_latest_save_version()\n",
    "\n",
    "    def _get_save_path(self) -> PurePosixPath:\n",
    "        if not self._version:\n",
    "            # When versioning is disabled, return original filepath\n",
    "            return self._filepath\n",
    "\n",
    "        save_version = self.resolve_save_version()\n",
    "        versioned_path = self._get_versioned_path(save_version)  # type: ignore\n",
    "\n",
    "        if self._exists_function(str(versioned_path)):\n",
    "            raise DataSetError(\n",
    "                f\"Save path '{versioned_path}' for {str(self)} must not exist if \"\n",
    "                f\"versioning is enabled.\"\n",
    "            )\n",
    "\n",
    "        return versioned_path\n",
    "\n",
    "    def _get_versioned_path(self, version: str) -> PurePosixPath:\n",
    "        return self._filepath / version / self._filepath.name\n",
    "\n",
    "    def load(self) -> _DO:\n",
    "        self.resolve_load_version()  # Make sure last load version is set\n",
    "        return super().load()\n",
    "\n",
    "    def save(self, data: _DI) -> None:\n",
    "        self._version_cache.clear()\n",
    "        save_version = self.resolve_save_version()  # Make sure last save version is set\n",
    "        try:\n",
    "            super().save(data)\n",
    "        except (FileNotFoundError, NotADirectoryError) as err:\n",
    "            # FileNotFoundError raised in Win, NotADirectoryError raised in Unix\n",
    "            _default_version = \"YYYY-MM-DDThh.mm.ss.sssZ\"\n",
    "            raise DataSetError(\n",
    "                f\"Cannot save versioned dataset '{self._filepath.name}' to \"\n",
    "                f\"'{self._filepath.parent.as_posix()}' because a file with the same \"\n",
    "                f\"name already exists in the directory. This is likely because \"\n",
    "                f\"versioning was enabled on a dataset already saved previously. Either \"\n",
    "                f\"remove '{self._filepath.name}' from the directory or manually \"\n",
    "                f\"convert it into a versioned dataset by placing it in a versioned \"\n",
    "                f\"directory (e.g. with default versioning format \"\n",
    "                f\"'{self._filepath.as_posix()}/{_default_version}/{self._filepath.name}\"\n",
    "                f\"').\"\n",
    "            ) from err\n",
    "\n",
    "        load_version = self.resolve_load_version()\n",
    "        if load_version != save_version:\n",
    "            warnings.warn(\n",
    "                _CONSISTENCY_WARNING.format(save_version, load_version, str(self))\n",
    "            )\n",
    "\n",
    "    def exists(self) -> bool:\n",
    "        \"\"\"Checks whether a data set's output already exists by calling\n",
    "        the provided _exists() method.\n",
    "\n",
    "        Returns:\n",
    "            Flag indicating whether the output already exists.\n",
    "\n",
    "        Raises:\n",
    "            DataSetError: when underlying exists method raises error.\n",
    "\n",
    "        \"\"\"\n",
    "        self._logger.debug(\"Checking whether target of %s exists\", str(self))\n",
    "        try:\n",
    "            return self._exists()\n",
    "        except VersionNotFoundError:\n",
    "            return False\n",
    "        except Exception as exc:  # SKIP_IF_NO_SPARK\n",
    "            message = (\n",
    "                f\"Failed during exists check for data set {str(self)}.\\n{str(exc)}\"\n",
    "            )\n",
    "            raise DataSetError(message) from exc\n",
    "\n",
    "    def _release(self) -> None:\n",
    "        super()._release()\n",
    "        self._version_cache.clear()\n",
    "\n",
    "\n",
    "def _parse_filepath(filepath: str) -> Dict[str, str]:\n",
    "    \"\"\"Split filepath on protocol and path. Based on `fsspec.utils.infer_storage_options`.\n",
    "\n",
    "    Args:\n",
    "        filepath: Either local absolute file path or URL (s3://bucket/file.csv)\n",
    "\n",
    "    Returns:\n",
    "        Parsed filepath.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        re.match(r\"^[a-zA-Z]:[\\\\/]\", filepath)\n",
    "        or re.match(r\"^[a-zA-Z0-9]+://\", filepath) is None\n",
    "    ):\n",
    "        return {\"protocol\": \"file\", \"path\": filepath}\n",
    "\n",
    "    parsed_path = urlsplit(filepath)\n",
    "    protocol = parsed_path.scheme or \"file\"\n",
    "\n",
    "    if protocol in HTTP_PROTOCOLS:\n",
    "        return {\"protocol\": protocol, \"path\": filepath}\n",
    "\n",
    "    path = parsed_path.path\n",
    "    if protocol == \"file\":\n",
    "        windows_path = re.match(r\"^/([a-zA-Z])[:|]([\\\\/].*)$\", path)\n",
    "        if windows_path:\n",
    "            path = \":\".join(windows_path.groups())\n",
    "\n",
    "    options = {\"protocol\": protocol, \"path\": path}\n",
    "\n",
    "    if parsed_path.netloc:\n",
    "        if protocol in CLOUD_PROTOCOLS:\n",
    "            host_with_port = parsed_path.netloc.rsplit(\"@\", 1)[-1]\n",
    "            host = host_with_port.rsplit(\":\", 1)[0]\n",
    "            options[\"path\"] = host + options[\"path\"]\n",
    "\n",
    "    return options\n",
    "\n",
    "\n",
    "def get_protocol_and_path(filepath: str, version: Version = None) -> Tuple[str, str]:\n",
    "    \"\"\"Parses filepath on protocol and path.\n",
    "\n",
    "    Args:\n",
    "        filepath: raw filepath e.g.: `gcs://bucket/test.json`.\n",
    "        version: instance of ``kedro.io.core.Version`` or None.\n",
    "\n",
    "    Returns:\n",
    "        Protocol and path.\n",
    "\n",
    "    Raises:\n",
    "        DataSetError: when protocol is http(s) and version is not None.\n",
    "        Note: HTTP(s) dataset doesn't support versioning.\n",
    "    \"\"\"\n",
    "    options_dict = _parse_filepath(filepath)\n",
    "    path = options_dict[\"path\"]\n",
    "    protocol = options_dict[\"protocol\"]\n",
    "\n",
    "    if protocol in HTTP_PROTOCOLS:\n",
    "        if version is not None:\n",
    "            raise DataSetError(\n",
    "                \"HTTP(s) DataSet doesn't support versioning. \"\n",
    "                \"Please remove version flag from the dataset configuration.\"\n",
    "            )\n",
    "        path = path.split(PROTOCOL_DELIMITER, 1)[-1]\n",
    "\n",
    "    return protocol, path\n",
    "\n",
    "\n",
    "def get_filepath_str(path: PurePath, protocol: str) -> str:\n",
    "    \"\"\"Returns filepath. Returns full filepath (with protocol) if protocol is HTTP(s).\n",
    "\n",
    "    Args:\n",
    "        path: filepath without protocol.\n",
    "        protocol: protocol.\n",
    "\n",
    "    Returns:\n",
    "        Filepath string.\n",
    "    \"\"\"\n",
    "    path = path.as_posix()\n",
    "    if protocol in HTTP_PROTOCOLS:\n",
    "        path = \"\".join((protocol, PROTOCOL_DELIMITER, path))\n",
    "    return path\n",
    "\n",
    "\n",
    "def validate_on_forbidden_chars(**kwargs):\n",
    "    \"\"\"Validate that string values do not include white-spaces or ;\"\"\"\n",
    "    for key, value in kwargs.items():\n",
    "        if \" \" in value or \";\" in value:\n",
    "            raise DataSetError(\n",
    "                f\"Neither white-space nor semicolon are allowed in '{key}'.\"\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro",
   "language": "python",
   "name": "kedro"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
